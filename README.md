## ðŸ›¡ï¸ LoLTrackGuard: Detecting Scripted Behavior in LoL

**LoLTrackGuard** is a lightweight system for detecting suspicious mouse behavior in *League of Legends* gameplay videos. It combines YOLOv8-based cursor detection with an LSTM autoencoder to identify anomalies in cursor movement patterns â€” no game logs or invasive tools required.

- ðŸŽ¯ Input: 1080p 30FPS gameplay video
- ðŸ–±ï¸ Step 1: Detect cursor positions using a trained YOLO model
- ðŸ“ Step 2: Extract and normalize motion features (velocity, acceleration, etc.)
- ðŸ§  Step 3: Feed into LSTM autoencoder to score anomalies
- ðŸ“Š Output: CSV with per-second anomaly scores

Trained on real pro player data, LoLTrackGuard offers a non-intrusive way to flag potential automation behavior in gameplay footage.

---

## ðŸ“ Project Structure

```bash
LoLTrackGuard-MAIN/
â”œâ”€â”€ cursor_templates/                # Cursor icon PNGs with transparency (for FakeDataGenerator)
â”œâ”€â”€ model/                           # Trained LSTM models for anomaly detection
â”‚   â”œâ”€â”€ detection_model.keras        # Default trained LSTM model
â”‚   â”œâ”€â”€ detection_model2.keras       # Alternate model versions
â”‚   â””â”€â”€ detection_model3.keras
â”œâ”€â”€ mouse_positions/                # Output CSVs from cursorDetector with raw mouse position data
â”œâ”€â”€ pipeline/                       # Core logic scripts
â”‚   â”œâ”€â”€ analyzer.py                 # Runs analysis using a trained model
â”‚   â”œâ”€â”€ cursorDetector.py          # Detects cursor in videos using YOLO and outputs CSV
â”‚   â”œâ”€â”€ dataModifier.py            # Extracts motion features and normalizes them
â”‚   â””â”€â”€ modelTrainer.py            # Trains LSTM anomaly detection model
â”œâ”€â”€ utils/                          # Resource files and utility scripts
â”‚   â”œâ”€â”€ cursorDetector_x.pt        # Primary YOLOv8 model for cursor detection
â”‚   â”œâ”€â”€ FakeDataGenerator.py       # Script to generate synthetic training data for YOLO
â”‚   â””â”€â”€ universal_scaler.joblib    # Saved standardizer for feature normalization
â”œâ”€â”€ train_pipeline.py               # Full training pipeline: from video to trained model
â”œâ”€â”€ analyze_pipeline.py             # Full analysis pipeline: from video to anomaly scores
â”œâ”€â”€ requirements.txt                # Python dependencies
â””â”€â”€ README.md                       # Project documentation
```

---

## âš™ï¸ Project Setup

### 1. Install Git LFS (for large model files)

```bash
# Install Git LFS from: https://git-lfs.github.com/
git lfs install
```

### 2. Install dependencies

```bash
pip install -r requirements.txt
```

---

## ðŸ” How to Use: Analysis Pipeline (`analyze_pipeline.py`)

### ðŸŽ¯ Purpose

Detect anomalies in mouse movement from gameplay recordings using a pre-trained model.

> âš ï¸ **Important:** Your input video **must be in 1080p 30 FPS** to ensure proper cursor detection and feature alignment.

### ðŸš€ Run

```bash
python analyze_pipeline.py
```

### ðŸ“Š Flow

```
1. Select a video file (e.g. MP4 screen recording with visible cursor)
2. Run YOLOv8 to detect and record mouse positions (X, Y, time)
3. Automatically extract movement features (velocity, acceleration, etc.)
4. Apply the pre-trained scaler to normalize features
5. Feed sequences into LSTM autoencoder
6. Calculate reconstruction error for each 1-second action
7. Save anomaly scores to CSV
```

### ðŸ“‚ Output

- `analysis_results/`: Contains CSV files listing the reconstruction error per action  
- Each row corresponds to a 1-second sequence (30 frames), indicating anomaly level


## ðŸ§  Train Your Own Model

If you want to train your own LSTM autoencoder model:

1. Use `pipeline/dataModifier.py` to extract features from your raw mouse position CSV files.
2. Use `utils/universal_scaler.joblib` to normalize the feature vectors.
3. Use `pipleline/modelTrainer.py` to train a new model on the processed sequences.

> ðŸ’¡ **Tip**: Make sure your input videos are consistently in **1080p 30 FPS**.

---

## ðŸ§ª How This Works

### 1. Cursor Detection via YOLOv8 and Synthetic Data

**Why Cursor?**
The goal of this project is to analyze first-person gameplay footage from streamers or content creators. Compared to character behavior, mouse cursor trajectories provide a more direct and reliable signal for detecting potential scripting.

Collected **mouse pointer** files and **replays of games** as backgrounds

Used `FakeDataGenerator.py` to generate over **70,000 labeled synthetic images**:
- Each frame is overlaid with a randomly selected cursor template
- Cursor size, brightness, saturation, and global blur are randomized

These noisy but labeled images were then used to train a **YOLOv8-based object detection model** capable of detecting mouse positions in real gameplay videos.

![cursor detection example](utils/img/1.png)
---

### 2. Behavioral Modeling via LSTM Autoencoder

To avoid subjective judgment in identifying cheaters, use an **LSTM autoencoder** trained purely on **verified human data** (non-cheating matches).

The model learns to reconstruct **normal human mouse movement patterns**. During inference, it flags any sequences with high **reconstruction error** as potential anomalies, without requiring manual rule definitions.

![LSTM](utils/img/2.png)

---

### 3. Feature Engineering with Real Pro Player Data

Collected **50 first-person replays from professional LoL players**, extracting over **1.5 million mouse movements**.

The raw cursor coordinates are processed using `dataModifier.py`, which:
- Extracts engineered features from raw (X, Y) data
- Replaces absolute timestamps with time deltas
- Computes per-frame velocity, acceleration (X/Y), angular velocity, and movement distance
- Applies `universal_scaler.joblib` for normalization

The movements are segmented into **sequences of 30 steps each** (1 second of motion at 30 FPS), representing atomic user actions.

These 500,000+ action sequences are fed into the LSTM for training.

---

### 4. Result Evaluation via Reconstruction Error

After running the full analysis pipeline, the `analyzer.py` script processes the extracted feature sequences using the trained LSTM autoencoder.

For each action (a 1-second sequence of mouse movement), the model calculates a **reconstruction error**:

- **Low error** â†’ behavior is similar to learned human patterns
- **High error** â†’ behavior is abnormal and potentially scripted or assisted

This allows for quantitative, objective evaluation of suspicious gameplay behavior.

The results are saved to `analysis_results/` as CSV files, where each row corresponds to one detected action with its associated anomaly score.

![Pro Player Data (Unseen During Training](utils/img/3.png)
![Regular Player Data](utils/img/4.png)
![Suspicious Gameplay Data](utils/img/5.png)